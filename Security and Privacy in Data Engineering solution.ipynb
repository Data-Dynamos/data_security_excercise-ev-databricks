{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a871e723-9ac4-4ead-bc7f-77457d2d0c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Security & Privacy in Workflows\n",
    "\n",
    "In this notebook, we'll explore ways to incorporate data security and privacy into large-scale data workflows. First, you might be wondering why we need to worry about these topics. Shouldn't this be a problem solved by the privacy department, infosec or via product owners? \n",
    "\n",
    "You can think of large scale data workflows like folks who manage the internet. We don't often see their work, but we know when it's broken! They probably deserve a lot more credit and attention for it, but we somehow just expect it \"to work ubiquitously.\" And we certainly expect the data we send around the internet to be kept private and secure (although in some geographies it is less likely...). If it wasn't for the work on those large-scale packet pipelines, then we couldn't trust technologies like SSL, TLS or our web applications or mobile applications. Those are enabled, propogated and enforced by all the intermediary hops, meaning the packet and data are handled with the same promises as they arrived. Hopefully you are getting the picture here -- security and privacy have to be baked into the architecture and data flow from the start, and cannot be simply \"tacked on\" at a given endpoint.\n",
    "\n",
    "So now we understand our responsibilities as the folks building the data backbones. What privacy and security concerns do we actually have? We'll walk through a concrete example to have a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b849cf-2e4e-47ed-8e8c-113ef197af08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example Data Product: Ingest Air Quality Sensors while Protecting User Privacy\n",
    "\n",
    "- We want to ingest air quality sensor data from users, buildings and institutions who are willing to send us data to build an air quality map (similar to the [IQAir map](https://www.iqair.com/air-quality-map).\n",
    "- Users only want to share the data if they can remain anonymous and their location is fuzzy, so that they are protected against stalkers, prying eyes and state surveillance.\n",
    "- Since the data is sensitive (from people and their homes!), we want to sure that it is secured either at collection, as well as at any intermediary hops.\n",
    "\n",
    "Let's first take a look at our data and determine what can and should be done..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f14680f-aaa0-453a-a43b-d7e0b0015284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93f364d-c4df-4ef4-a9b0-a89af6b7fbe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, lit, col\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "df = spark.read.csv(f'file:{cwd}/data/air_quality.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d750314-43d6-44ab-9e11-ab03034ba161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e742d6-0ec0-40f0-bfa1-d9082be7c71f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('location').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749cadfc-183b-4cca-bfff-f19b1336de13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de3687e0-21f4-49bb-8ae0-e10937c189b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Remove some of the extra characters around the string so that it is easier to parse using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eabbd1a-9843-473e-8610-93ee63a389e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 1 - To remove extra characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4173e2ab-331c-4f8a-8318-9dfbbee1e206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_brackets_in_location(df): \n",
    "# remove extra characters - \"[()']\" from the location column string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c6dfb7-2ff7-48ef-985b-bc333a932c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def replace_brackets_in_location(df): \n",
    "    return df.withColumn(\"location\", F.regexp_replace(\"location\", \"[()']\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01270763-495d-41ff-9cde-168dbb491432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_replace_brackets_in_location():\n",
    "  pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'location': pd.Series([\"New York, NY\", \"(Los Angeles), CA\", \"Chicago' IL\"]),\n",
    "      }\n",
    "  )\n",
    "  expected_pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'location': pd.Series([\"New York, NY\", \"Los Angeles, CA\", \"Chicago IL\"]),\n",
    "      }\n",
    "  )\n",
    "  spark_df = spark.createDataFrame(pandas_df)\n",
    "  expected_df = spark.createDataFrame(expected_pandas_df)\n",
    "\n",
    "  fixed_df = replace_brackets_in_location(spark_df)\n",
    "  \n",
    "  assert fixed_df.schema == expected_df.schema\n",
    "  assert fixed_df.collect() == expected_df.collect()\n",
    "\n",
    "  print(\"All tests passed :)\")\n",
    "  \n",
    "test_replace_brackets_in_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4022e4-01c5-4ac3-9212-22e7be61c818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = replace_brackets_in_location(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2e1afb-9e59-440e-a92d-dddee62809a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('location').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a375bb-2031-4494-9df4-c57c8d278104",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Split the location string to properly parse it. Spark has a split function that can help. Put this into a new column called location_arr for location array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8fe9ffe-1660-4cdb-808c-1e23427eb03e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 2 - To split the \"location\" column into an array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a079e6-6553-4da2-af24-be32b4d7e745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_location(df):\n",
    "# split the \"location\" column into an array of strings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e045652a-c902-4120-866e-1f71907d23d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def split_location(df):\n",
    "    return df.withColumn(\"location_arr\", F.split(\"location\", \", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0752a27d-5bb4-4a11-a286-092b006e0fc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.types import IntegerType, StructField, StringType, ArrayType, FloatType, StructType\n",
    "\n",
    "def test_split_location():\n",
    "  schema = StructType([\n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"location\", StringType())\n",
    "  ])\n",
    "  pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'location': pd.Series([\"New York, NY\", \"Los Angeles, CA\", \"Chicago, IL\"]),\n",
    "      }\n",
    "  )\n",
    "  expected_schema = StructType([\n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"location\", StringType()),\n",
    "      StructField(\"location_arr\", ArrayType(StringType(), False))\n",
    "  ])\n",
    "  expected_pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'location': pd.Series([\"New York, NY\", \"Los Angeles, CA\", \"Chicago, IL\"]),\n",
    "          'location_arr': pd.Series([ [\"New York\",\"NY\"], [\"Los Angeles\",\"CA\"], [\"Chicago\",\"IL\"]]),\n",
    "      }\n",
    "  )\n",
    "  spark_df = spark.createDataFrame(pandas_df,schema)\n",
    "  expected_df = spark.createDataFrame(expected_pandas_df,expected_schema)\n",
    "\n",
    "  fixed_df = split_location(spark_df)\n",
    " \n",
    "  assert fixed_df.schema == expected_df.schema\n",
    "  assert fixed_df.collect() == expected_df.collect()\n",
    "\n",
    "  print(\"All tests passed :)\")\n",
    "  \n",
    "test_split_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d60bf8-953a-41e7-8160-d9cb9d1e2b52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = split_location(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064940ed-62a3-41d6-a202-c02524003656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae720ff-d6ad-41e2-bbba-3750c8c0b996",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To fix the schema problems... split the location array and put them into actual columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d2d253-0725-487f-b8b6-0629ed32d195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 3 - The 'location' column should be split into separate columns lat,long,city,country and timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b729b1a-3dff-4f36-8026-4509e8ba8282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_cols_for_location_arr_items(df):\n",
    "# The 'location' column should be split into separate columns lat,long,city,country and timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6ba72e-91ad-4031-b4fc-3a4381daede4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def create_cols_for_location_arr_items(df):\n",
    "    return df.withColumn('lat', df.location_arr.getItem(0).cast('float')) \\\n",
    "    .withColumn('long', df.location_arr.getItem(1).cast('float')) \\\n",
    "    .withColumn('city', df.location_arr.getItem(2)) \\\n",
    "    .withColumn('country', df.location_arr.getItem(3)) \\\n",
    "    .withColumn('timezone', df.location_arr.getItem(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bd5e6d-0275-441c-88ee-f041d6b3553a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_create_cols_for_location_arr_items():\n",
    "  schema = StructType([\n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"location\", StringType()),\n",
    "       StructField(\"location_arr\", ArrayType(StringType(), False))\n",
    "  ])\n",
    "  pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2]),\n",
    "          'location': pd.Series([\"New York, NY\", \"Los Angeles, CA\"]),\n",
    "          'location_arr' : pd.Series([ [\"40.7128\",\"-74.0060\",\"New York\",\"USA\",\"America/New_York\"], [\"34.0522\",\"-118.2437\",\"Los Angeles\",\"USA\",\"America/Los_Angeles\"]])\n",
    "      }\n",
    "  )\n",
    "  expected_schema = StructType([\n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"location\", StringType()),\n",
    "      StructField(\"location_arr\", ArrayType(StringType(), False)),\n",
    "      StructField(\"lat\", FloatType()),\n",
    "      StructField(\"long\", FloatType()),\n",
    "      StructField(\"city\", StringType()),\n",
    "      StructField(\"country\", StringType()),\n",
    "      StructField(\"timezone\", StringType())\n",
    "  ])\n",
    "  expected_pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2]),\n",
    "          'location': pd.Series([\"New York, NY\", \"Los Angeles, CA\"]),\n",
    "          'location_arr' : pd.Series([ [\"40.7128\",\"-74.0060\",\"New York\",\"USA\",\"America/New_York\"], [\"34.0522\",\"-118.2437\",\"Los Angeles\",\"USA\",\"America/Los_Angeles\"]]),\n",
    "          'lat' : pd.Series([40.7128, 34.0522]),\n",
    "          'long' : pd.Series([-74.0060, -118.2437]),\n",
    "          'city' : pd.Series([\"New York\", \"Los Angeles\"]),\n",
    "          'country' : pd.Series([\"USA\", \"USA\"]),\n",
    "          'timezone' : pd.Series([\"America/New_York\", \"America/Los_Angeles\"]),\n",
    "      }\n",
    "  )\n",
    "  spark_df = spark.createDataFrame(pandas_df,schema)\n",
    "  expected_df = spark.createDataFrame(expected_pandas_df,expected_schema)\n",
    "\n",
    "  fixed_df = create_cols_for_location_arr_items(spark_df)\n",
    " \n",
    "\n",
    "  assert fixed_df.schema == expected_df.schema\n",
    "  assert fixed_df.collect() == expected_df.collect()\n",
    "\n",
    "  print(\"All tests passed :)\")\n",
    "  \n",
    "test_create_cols_for_location_arr_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029fda7b-3237-4f06-afc2-dffca6f3ab3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = create_cols_for_location_arr_items(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08ea923d-09d1-46c3-a558-1c3e89ef259e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The schema above looks much better, drop the old columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e15e70-135b-4599-82d5-4ad6f73a4db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('location', 'location_arr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a06f2e9b-8e90-4cb5-9dc3-ff4ae84d0f25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next I need to create my air quality categories as follows...\n",
    "\n",
    "Based on the IQAir map, the ranges look to be about:\n",
    "\n",
    "    Great: less than or equal to 50\n",
    "    Good: 51-100\n",
    "    Okay: 101-150\n",
    "    Poor: 151-200\n",
    "    Bad: 201-300\n",
    "    Extremely Bad: 301+\n",
    "\n",
    "Let's make these into integer values 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99446385-b05f-4edc-989d-4f91ad292be1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('air_quality_category', F.when(\n",
    "    df.air_quality_index <= 50, 1).when(\n",
    "    df.air_quality_index <= 100, 2).when(\n",
    "    df.air_quality_index <= 150, 3).when(\n",
    "    df.air_quality_index <= 200, 4).when(\n",
    "    df.air_quality_index <= 300, 5).otherwise(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309c7c50-55dc-4b45-b450-1f7b9a850ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabb8200-8a86-4a3d-b77f-d59f6ec8f239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### So what even is sensitive information?\n",
    "\n",
    "Categories of sensitive information:\n",
    "\n",
    "- **Personally Identifiable Information (PII)**: This is information that we can directly link to a person without much effort. This includes information like email address, IP address, legal name, address, birth date, gender and so forth. Even just one of these pieces of information can be enough to directly identify someone in a dataset.\n",
    "- **Person-Related Information**: This is data that is created by a person and that likely has some personal artifacts. For example, [web browsing histories are fairly unique](https://blog.lukaszolejnik.com/web-browsing-histories-are-private-personal-data-now-what/), so is location data (i.e. Where do you sleep at night? Where do you work?) and even your likes on social media can statistically reveal sensitive attributes, such as your gender, ethnicity and your political preferences.\n",
    "- **Confidential Information**: This is sensitive information for companies, that should remain protected via similar methods as personal data. This data could reveal details about the core business model, proprietary practices, customer details (which can also contain personal information!) and internal business processes.\n",
    "\n",
    "When we define sensitive information as only PII, we tend to ignore other potential targets of sensitive data, that might be just as, if not more valuable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d53bbe-8f60-4ccd-89a2-23e7a2b5317c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is sensitive here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5a9e27-34bd-48d9-b1b8-0760f8dcbf18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sample(0.01).show(3, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ae697d-4040-46e5-a92d-e9f79b03dcae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### How might we...?\n",
    "\n",
    "- Protect user_id while still allowing it to be linkable?\n",
    "- Remove potentially identifying precision in location?\n",
    "- Remove potentially identifying information in the timestamp?\n",
    "- Make these into scalable and repeatable actions for our workflow?\n",
    "\n",
    "Let's work on these step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ce1ccb-12a9-402d-8b35-77aefebb714e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cryptographic Hashing\n",
    "\n",
    "\n",
    "- Protect user_id by hashing it's value \n",
    "\n",
    "Hashing is a one-way function that takes an input (e.g. a password or other data) and generates a fixed-size output (the hash value) that is unique to that input.The purpose of hashing is to provide data integrity and to protect against tampering or unauthorized modifications of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a880a0-3849-475d-96d0-62eaf5791a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The hashlib module in python implements a common interface for many secure cryptographic hash and message digest algorithms.\n",
    "# Example code to show how hashlib works and hashes a string. \n",
    "import hashlib\n",
    "hashlib.sha256(b\"Nobody inspects the spammish repetition\").hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98153ac-ec16-4076-aba8-b24645f9c7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 4 - Define function to hash column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3d74c3-39cd-4764-99ef-3ba37c12148b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def hash_column(df, column_name):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca81e7d-7b3f-4124-a025-c45b9e13daad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#solution\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import hashlib\n",
    "\n",
    "def hash_column(df, column_name):\n",
    "    def sha256_hash(value):\n",
    "        return hashlib.sha256(str(value).encode('utf-8')).hexdigest()\n",
    "    udf_sha256_hash = udf(sha256_hash, StringType())\n",
    "    df = df.withColumn('hashed_' + column_name, udf_sha256_hash(column_name))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecc5a6e-38e9-4268-bf79-841d6e8ee9b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_hash_column():\n",
    "    schema = StructType([StructField(\"user_id\", StringType(),True)])\n",
    "    pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'user_id':  pd.Series([\"12345\", \"67890\", \"abcdef\"])\n",
    "      }\n",
    "    )\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"user_id\", StringType(),True),\n",
    "        StructField(\"hashed_user_id\", StringType(),True)\n",
    "    ])\n",
    "    expected_pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'user_id':  pd.Series([\"12345\", \"67890\", \"abcdef\"]),\n",
    "          'hashed_user_id':  pd.Series(\n",
    "              [\"5994471abb01112afcc18159f6cc74b4f511b99806da59b3caf5a9c173cacfc5\", \n",
    "               \"e2217d3e4e120c6a3372a1890f03e232b35ad659d71f7a62501a4ee204a3e66d\", \n",
    "               \"bef57ec7f53a6d40beb640a780a639c83bc29ac8a9816f1fc6c5c6dcd93c4721\"\n",
    "              ])\n",
    "      }\n",
    "    )    \n",
    "    df = spark.createDataFrame(pandas_df,schema)\n",
    "    expected_df = spark.createDataFrame(expected_pandas_df,expected_schema)\n",
    "    \n",
    "    hashed_df = hash_column(df, \"user_id\")\n",
    "\n",
    "    assert hashed_df.schema == expected_df.schema\n",
    "    assert hashed_df.collect() == expected_df.collect()\n",
    "    print(\"All tests passed :)\")\n",
    "    \n",
    "test_hash_column()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0bd939-4c90-4c64-b180-e267f6dbb58c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = hash_column(df, 'user_id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a18a57-49ca-4c89-9535-2650bb3baed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df= spark.createDataFrame(df.rdd, df.schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d8bfaf-ae4e-4ac0-9446-e14604891ece",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b2f0e3-98a7-4377-bda0-99b5b0e2c616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The fernet module of the cryptography package has inbuilt functions for the generation of the key, encryption of plaintext into ciphertext, and decryption of ciphertext into plaintext using the encrypt and decrypt methods respectively. The fernet module guarantees that data encrypted using it cannot be further manipulated or read without the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0e1452-4738-419c-8f97-55fe1eb7699a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example code to show how Fernet works and encrypts a string.  \n",
    "from cryptography.fernet import Fernet\n",
    "# >>> Put this somewhere safe!\n",
    "key = Fernet.generate_key()\n",
    "f = Fernet(key)\n",
    "token = f.encrypt(b\"secret message\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6df7b17-76a3-48d2-84ab-251b6b914665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f.decrypt(token).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6280854e-a3f8-4c19-b30c-be63014f8199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate the encryption key\n",
    "Key = Fernet.generate_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40b0a21-c6ea-4daa-9ff2-e635ee5dd054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccb93c0-7fb4-4ec7-b76a-dbb15b966153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 5 - Define Encrypt User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8bf483e-6c23-4a3f-8adb-21b3ce7d2485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Encrypt User Defined Function \n",
    "def encrypt_user_id(plain_text,MASTER_KEY):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2875d4ac-05f8-4f28-8249-e7a764861822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def encrypt_user_id(plain_text,MASTER_KEY):\n",
    "    f = Fernet(MASTER_KEY)\n",
    "    plain_text_bytes=bytes(plain_text, 'utf-8')\n",
    "    cipher_text = f.encrypt(plain_text_bytes)\n",
    "    cipher_text = str(cipher_text.decode('ascii'))\n",
    "    return cipher_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efed140-46bd-4658-82ce-34ab8d247e07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_encrypt_user_id():\n",
    "    \n",
    "    plain_df = spark.createDataFrame(\n",
    "        data=[\n",
    "            (1,\"abcd\"),\n",
    "            (2,\"efgh\"),\n",
    "            (3,\"ijkl\")\n",
    "        ],\n",
    "        schema= [\"id\",\"user_id\"],\n",
    "    )\n",
    "       \n",
    "    key = Fernet.generate_key()\n",
    "    f = Fernet(key)\n",
    "    \n",
    "    encrypted_udf = udf(encrypt_user_id, StringType())    \n",
    "    encrypted_df = plain_df.withColumn(\"user_id\", encrypted_udf(col('user_id'), lit(key))) \n",
    "    \n",
    "    expected_decrypted_list = [f.decrypt(i['user_id'].encode()).decode() for i in encrypted_df.select(col('user_id')).collect() ]\n",
    "    \n",
    "    assert plain_df.select(col('user_id')).collect() != encrypted_df.select(col('user_id')).collect(),\"The user_id column has to be encrypted after the encryption logic applied\"    \n",
    "    assert plain_df.rdd.map(lambda x: x.user_id).collect() == expected_decrypted_list,\"The decrypted values has to match the plain_df's user_id values\"\n",
    "    \n",
    "    print(\"All tests passed :)\")\n",
    "    \n",
    "test_encrypt_user_id() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff73fa3-c77f-45bc-984b-9535673e4d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encrypted_udf = udf(encrypt_user_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497fa546-0468-4228-b38d-f058fc1fd093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(encrypted_udf(col(\"user_id\"),lit(Key)).alias(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52cdf63-56b6-499c-90d6-9e276984d5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like it's working, but with UDFs you never know. Remember, Spark function evaluation is LAZY, so it will sample a bit and test. To see if it will work on the entire dataframe, we need to call collect. Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce779e8a-3b5f-4524-b2c2-2214c660b17c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = df.select(encrypted_udf(col(\"user_id\"),lit(Key)).alias(\"user_id\")).collect()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c266e1-20ce-4400-bab5-2c9fd83930b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This looks like it works now! Let's add it as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a7ea42-5565-45a8-a52f-72c4a013897c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"user_id\", encrypted_udf(col('user_id'), lit(Key))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eaf73a-d931-48a1-afa5-80e8916766f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d3f15b-b55f-4cfa-847f-9d0c17624d1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Also we could write a function to decrypt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e3f691-b2cd-4ed3-b8d0-30fb845d4584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define decrypt user defined function \n",
    "def decrypt_user_id(cipher_text,MASTER_KEY):\n",
    "    from cryptography.fernet import Fernet\n",
    "    f = Fernet(MASTER_KEY)\n",
    "    clear_val=f.decrypt(cipher_text.encode()).decode()\n",
    "    return clear_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd428f2-bad4-4613-9f5e-7109a4c255a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "decrypted_udf = udf(decrypt_user_id, StringType())\n",
    "dec_df = df.withColumn(\"user_id\", decrypted_udf(col('user_id'), lit(Key))) \n",
    "dec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a7797b-0ab2-4181-bf23-6ee8b982e701",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Format-Preserving Encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecbcf314-272a-401b-84d7-1b26be04216f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is FPE?\n",
    "\n",
    "Format-preserving encryption (FPE) is the process of encrypting data in such a way that the output (ciphertext) remains in the same format as the input (plaintext). The meaning of “format” varies. Typically only finite sets of characters are used; numeric, alphabetic or alphanumeric. \n",
    "\n",
    "For example:\n",
    "\n",
    "- A 9-digit social security number is encrypted into a 9-digit ciphertext string\n",
    "- A 16-digit credit card number is encrypted into a 16-digit ciphertext string\n",
    "- A 10-digit phone number is encrypted into a 10-digit ciphertext string\n",
    "\n",
    "Reference : https://www.ubiqsecurity.com/what-is-format-preserving-encryption-fpe-and-its-benefits%EF%BF%BC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8e93a7-4f83-487d-a7db-86329b063e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyffx is a pure Python implementation of Format-preserving, Feistel-based encryption (FFX).\n",
    "# Example code to show how pyffx works and encrypts a string. \n",
    "import pyffx\n",
    "e = pyffx.String(b'secret-key', alphabet='abc', length=6)\n",
    "e.encrypt('aaabbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d506b6-350d-447b-99be-55bf6b0bfa3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "e.decrypt('acbacc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63acbdb1-9a21-457f-a36b-d4cf73646095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 6 - Define Format Preserving Encryption Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3241e12-eb20-4bdb-b93d-75b117c05e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the Format Preserving Encryption, we are now technically leaking length information... which we could determine is okay, so long as access to this data and the real data is fairly controlled. \n",
    "We could also say that we want to by default add padding to every username to make them consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11caf936-9656-4b4e-a31b-ddd947ff82b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import pyffx\n",
    "\n",
    "def add_padding(value):\n",
    "    if len(value) < 6:\n",
    "        value += \"X\" * (6-len(value))\n",
    "    return value\n",
    "\n",
    "def ffx_encrypt(key, value):\n",
    "#     TODO : Add padding to the value and perform format preserving encryption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ea3abe-c659-4e04-ad31-8525c5b8ef26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import pyffx\n",
    "\n",
    "def add_padding(value):\n",
    "    if len(value) < 6:\n",
    "        value += \"X\" * (6-len(value))\n",
    "    return value\n",
    "\n",
    "def ffx_encrypt(key, value):\n",
    "    padded_value = add_padding(value)\n",
    "    ffx = pyffx.String(key.encode(), alphabet='abcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', length=len(padded_value))\n",
    "    return ffx.encrypt(padded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "444bed13-d8ac-4bd5-90e9-98509807daa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fpe_encrypt_column(df, column_name, key):\n",
    "    ffx_udf = udf(lambda x: ffx_encrypt(key, x), StringType())\n",
    "    df = df.withColumn(\"fpe_encrypted_\" + column_name, ffx_udf(col(column_name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa9730f-8abd-4b7e-8ab0-fa1b656bcc4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def test_fpe_encrpyt_column():\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"name\", StringType())\n",
    "    ])\n",
    "    data = [\n",
    "        Row(\"1\", \"John\"),\n",
    "        Row(\"2\", \"Jane\"),\n",
    "        Row(\"3\", \"Bob\"),\n",
    "        Row(\"4\", \"Alice\")\n",
    "    ]\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"fpe_encrypted_name\", StringType())\n",
    "    ])\n",
    "    expected_result = [\n",
    "        ('1', \"John\", \"pYzuM5\"),\n",
    "        ('2', \"Jane\", \"Vlqeg2\"),\n",
    "        ('3', \"Bob\", \"BCDYUh\"),\n",
    "        ('4', \"Alice\", \"Wn9JRa\")\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_result, expected_schema)\n",
    "    key = 'secret_key'\n",
    "    \n",
    "    encrypted_df = fpe_encrypt_column(df, 'name', key)\n",
    "\n",
    "    assert encrypted_df.collect() == expected_df.collect() , \"The user_id column has to be encrypted after the encryption logic applied\" \n",
    "    print(\"All tests passed :)\")\n",
    "    \n",
    "test_fpe_encrpyt_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa6a49b-71d7-44b5-8a4f-830a284a2571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key = \"mysecretkey\"\n",
    "new_df = fpe_encrypt_column(new_df, \"user_id\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db01e976-4dcd-4f63-8b11-8011340339ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ee019e7-414c-4c71-a018-6a838959227b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Format-Preserving Decryption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb36727-d2ec-4fcf-869d-4178b82ea4e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 7 - Define Format Preserving Decryption Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5774558-2a86-4fb2-9464-79beac6836fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ffx_decrypt(key, value):\n",
    "#     TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597b5511-df0d-47b0-8931-f71b7b2ce2d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def ffx_decrypt(key, value):\n",
    "    ffx = pyffx.String(key.encode(), alphabet='abcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', length=len(value))\n",
    "    return ffx.decrypt(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a341992a-3923-4572-b430-b7db99c9a842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fpe_decrypt_column(df, column_name, key):\n",
    "    ffx_udf = udf(lambda x: ffx_decrypt(key, x), StringType())\n",
    "    df = df.withColumn(\"fpe_decrypted_user_id\", ffx_udf(col(column_name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81811dfa-7562-4770-b35d-b75919e6b1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def test_fpe_decrypt_column():\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"fpe_encrypted_user_id\", StringType())\n",
    "    ])\n",
    "    data = [\n",
    "        Row('1', \"pYzuM5\"),\n",
    "        Row('2', \"Vlqeg2\"),\n",
    "        Row('3', \"BCDYUh\"),\n",
    "        Row('4', \"Wn9JRa\")\n",
    "    ]\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"fpe_encrypted_user_id\", StringType()),\n",
    "        StructField(\"fpe_decrypted_user_id\", StringType()),\n",
    "    ])\n",
    "    expected_data = [\n",
    "        Row(\"1\", \"pYzuM5\", \"JohnXX\"),\n",
    "        Row(\"2\", \"Vlqeg2\", \"JaneXX\"),\n",
    "        Row(\"3\", \"BCDYUh\", \"BobXXX\"),\n",
    "        Row(\"4\", \"Wn9JRa\", \"AliceX\")\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_schema)\n",
    "    key = 'secret_key'\n",
    "\n",
    "    decrypted_df = fpe_decrypt_column(df, \"fpe_encrypted_user_id\",key)\n",
    "\n",
    "    assert decrypted_df.schema == expected_df.schema\n",
    "    assert decrypted_df.collect() == expected_df.collect()\n",
    "    \n",
    "    print(\"All tests passed :)\")\n",
    "test_fpe_decrypt_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d934a299-1dfb-49f5-b65f-d6da8f40fd54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_decrypted = fpe_decrypt_column(new_df ,\"fpe_encrypted_user_id\", key)\n",
    "df_decrypted.select(\"user_id\", \"fpe_decrypted_user_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70da94f-d79a-4a82-9321-c5f333fdd688",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This would be a good homework exercise to write a function to decrypt which also removes padding!!. One challenge, what happens if the username ends in X???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1a7e7b-0838-4b1c-8663-ff1d9fc7bf2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can move onto our GPS data!\n",
    "\n",
    "How precise is GPS data anyways? 🤔 (from [wikipedia](https://en.wikipedia.org/wiki/Decimal_degrees))\n",
    "\n",
    "\n",
    "decimal places  | degrees  |distance\n",
    "------- | -------          |--------\n",
    "0        |1                |111  km\n",
    "1        |0.1              |11.1 km\n",
    "2        |0.01             |1.11 km\n",
    "3        |0.001            |111  m\n",
    "4        |0.0001           |11.1 m\n",
    "5        |0.00001          |1.11 m\n",
    "6        |0.000001         |11.1 cm\n",
    "7        |0.0000001        |1.11 cm\n",
    "8        |0.00000001       |1.11 mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765bd860-0fd6-4c6f-9741-c405ee9b8c2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1484d325-e30f-4722-abc9-edd980b6e110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To reduce precision in Spark, take a look at F.pys_round !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6bf30b7-380c-4a52-9c2e-820624265624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 8 - To reduce column precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353bc260-b3e0-4642-9362-924213fb10e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO reduce precision for the lat and long columns!\n",
    "def remove_column_precision(df,column_name):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ecf232-f74a-44e9-b230-56fbf8c172d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "def remove_column_precision(df,column_name):\n",
    "    return df.withColumn(column_name, F.round(column_name, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806be8b4-4d5d-4230-a01c-a585d45f7607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_remove_column_precision():\n",
    "  pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'lat': pd.Series([40.1234567, 42.9876543, 45.67890]),\n",
    "      }\n",
    "  )\n",
    "  expected_pandas_df = pd.DataFrame(\n",
    "      {\n",
    "          'id':  pd.Series([1, 2, 3]),\n",
    "          'lat': pd.Series([40.123, 42.988, 45.679]),\n",
    "      }\n",
    "  )\n",
    "  spark_df = spark.createDataFrame(pandas_df)\n",
    "  expected_df = spark.createDataFrame(expected_pandas_df)\n",
    "\n",
    "  fixed_df = remove_column_precision(spark_df,'lat')\n",
    "\n",
    "  assert fixed_df.schema == expected_df.schema\n",
    "  assert fixed_df.collect() == expected_df.collect()\n",
    "\n",
    "  print(\"All tests passed :)\")\n",
    "  \n",
    "test_remove_column_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d8b083-9f34-45db-82fe-3647831f7289",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once you get the above working without error, you can fix the lat and long columns in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d5acfb1-51ec-4af3-a659-2cd42e1821d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = remove_column_precision(df,'lat')\n",
    "df = remove_column_precision(df,'long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e93cfc4-f19e-4667-973e-06669762d2c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What type of risk should we be aware of with regard to timestamp precision? When and how do we need to de-risk this  type of information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f836e490-7f82-4042-a182-0ad09425d774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('timestamp', F.to_timestamp('timestamp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd33a55-e8e5-4929-9932-06d22dacfa06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Try adding a new column called new_timestamp where you use F.unix_timestamp and F.rand to build a small amount of noise..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9aa0011-b5bf-4337-a5f8-a120cf25215f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 9 - To add random time intervals to timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed18a72-c0f5-491d-917a-2934c061a1fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, unix_timestamp, rand\n",
    "new_df = cleaned_df\n",
    "  .withColumn(\"new_timestamp\", #TODO: finish this to add the column to your DF once you have experimented and like the result!)\n",
    "  .select(col(\"timestamp\"), col(\"new_timestamp\"))\n",
    "\n",
    "new_df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b36ddf-6414-4354-b3c8-62fc37edfc16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Solution \n",
    "from pyspark.sql.functions import to_timestamp, unix_timestamp, rand\n",
    "df = df.withColumn(\"timestamp\", (unix_timestamp(col(\"timestamp\")) +\n",
    "                  (rand() * 60) + (rand() * 60 * 20)).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e1beed-58c7-41ae-9d03-3d5f33e85310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_add_random_time_intervals():\n",
    "    # create a sample DataFrame\n",
    "    df = spark.createDataFrame([\n",
    "        ('2022-01-01 00:00:00', 1),\n",
    "        ('2022-01-01 01:00:00', 2),\n",
    "        ('2022-01-01 02:00:00', 3),\n",
    "        ('2022-01-01 03:00:00', 4),\n",
    "        ('2022-01-01 04:00:00', 5),\n",
    "    ], ['timestamp', 'value'])\n",
    "    cleaned_df = df.withColumn('timestamp', (unix_timestamp('timestamp') + \n",
    "                            (rand() * 60) + (rand() * 60 * 20)).cast('timestamp'))\n",
    "    assert 'timestamp' in cleaned_df.columns\n",
    "    assert 'value' in cleaned_df.columns\n",
    "    assert cleaned_df.filter(cleaned_df.timestamp != df.timestamp).count() == df.count()\n",
    "    print(\"All tests Passed :)\")\n",
    "test_add_random_time_intervals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b569611a-5261-455a-9642-615ec62deba6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we want to reorder so the data comes into the map in the proper order (for timeseries analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76d5856-a256-4215-9a11-d9cde3f09ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.orderBy(df.timestamp.asc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9a24b2-9f22-4957-b9c8-99c3422d266a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv').save(f'{root_dir}/data/data_for_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a5771b-63f4-46d0-920c-812a8d1bb293",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/VijayaDurgaN/data-security-exercise/master/images/cia_triad.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de7fdef-ff40-4a00-8719-b0f5641bde9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is a graphic from Wikipedia showing the so-called \"CIA\" triad, showing some of the core concepts we want to ensure to guarantee data security. Let's review them together:\n",
    "\n",
    "- **Confidentiality:** Data is kept confidential, meaning only those who should be able to access it can do so, and fine-tuned access is available and enforced.\n",
    "- **Integrity:** Data is accurate and cannot easily be changed or tampered with by internal or external actors in a malicious way.\n",
    "- **Availability:** Data fulfills any service-level objectives (SLOs) or service-level agreements (SLAs) and is made available in a secure and user-friendly manner. \n",
    "\n",
    "So translated into data engineering context, this means that:\n",
    "\n",
    "- Our data workflows enforce access-control restrictions, data protections or minimizations related to confidentiality and ensure sinks and sources match the encryption requirements we expect for the data sensitivity.\n",
    "- Our data workflows do not mangle data, maintain data quality principles outlined by governance processes and alert should malicious activity be detected.\n",
    "- Our data wofkflows meet SLOs/SLAs outlined by the data consumers and dependant data products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adbd130-830b-4239-9527-651f18387dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What about Privacy? 🦹🏻\n",
    "\n",
    "A foundational concept when it comes to designing privacy-respecting systems is the Privacy by Design principles outlined by [Anne Cavoukian in 2006](https://iapp.org/media/pdf/resource_center/pbd_implement_7found_principles.pdf).\n",
    "\n",
    "Let's pull out a few of the principles that relate to our work as data engineers...\n",
    "\n",
    "- **Proactive not Reactive; Preventative not Remedial:** Privacy is built into our architecture and data flows as we start building them. Think of this as the privacy version of TDD -- we write the privacy requirements first and design and build systems to fit them, not the other way around!\n",
    "- **Privacy as the Default Setting:** We optimize systems so that privacy is on by default, and changes to that are user-driven! This means tracking things like consent, implementing processes for data minimization and ensuring lineage and governance data is available to data consumers or dependant data products.\n",
    "- **Full Functionality – Positive-Sum, not Zero-Sum:** Data privacy is a benefit for the business, technologists and users, meaning we ensure that it is not a tradeoff in our product design. Users who choose privacy protections (or users who have them on automatically, by default, right?) receive full functionality.\n",
    "- **End-to-End Security – Full Lifecycle Protection:** Data is secured properly and for it's entire lifecycle (from collection endpoint to deletion!). Here is our big intersection with the security requirements.\n",
    "\n",
    "\n",
    "What does this mean for our data engineering work?\n",
    "\n",
    "- Our data workflows have privacy protections outlined and architected in before any code is written. We test for these and ensure they are working properly, should anything change.\n",
    "- Privacy is turned on by default, and any \"unknown\" data flows have privacy added to them when they enter into our systems or are discovered (e.g. in cases of unknown data storages or data from third parties).\n",
    "- We work directly with data producers and consumers (and other stakeholders, such as legal or privacy professionals) to find sweet spots that offer the appropriate protection for users and utility for business needs. Approach this as a postive-sum game and remember that user-centric choices are always a good business investment.\n",
    "- We design secure workflows that ensure that all user-related or person-related data is properly secured using standards from data security best practices (like our CIA triad!)\n",
    "\n",
    "\n",
    "#### Privacy and Information Continuum\n",
    "\n",
    "One useful way to begin shifting your understanding of privacy is to start thinking about it as a point on a spectrum instead of something that is \"on\" or \"off\". Here we can see that we can have a full range of points on a continuum, where privacy and information are somewhat at odds with one another. When we have full information, we have no privacy guarantees. When we have complete privacy, we cannot do our job as data people! Finding the right balance is the difficult and fun part of privacy in data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b669039c-c6e5-4164-9f44-9d823c86319e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/VijayaDurgaN/data-security-exercise/master/images/privacy_and_information_continuum.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6983e4-ff33-4fa8-a51d-15e25e4c94b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Congratulations!! \n",
    "\n",
    "You've walked through potential privacy snags and helped increase the protection for the individuals sending you their air quality details! Now developers can use this dataset and we have ensured that there are some base protections. As you may have noticed, it wasn't always obvious what we should do -- but by thinking through each data type and determining what worked to balance the utility of the data and the privacy we want to offer, we were able to find some ways to protect individuals. \n",
    "\n",
    "A good set of questions to ask for guidance is:\n",
    "\n",
    "- Where will this data be accessed and used? How safe is this environment?\n",
    "- What person-related data do we actually need to use to deliver this service or product? (data minimization!)\n",
    "- What other protections will be added to this data before it is seen or used? (i.e. encryption at rest, access control systems, or other protections when it reaches another processing point or sink!)\n",
    "- What privacy and security expectations do we want to set for the individuals in this dataset?\n",
    "- Where can we opportunistically add more protection while not hindering the work of data scientists, data analysts, software engineers and other colleagues?\n",
    "\n",
    "\n",
    "As you continue on in your data engineering journey, you'll likely encounter many more situations where you'll need to make privacy and security decisions. If you'd like to learn more and even work as a privacy or security champion -- feel free to join in your organizations' programs to support topics like this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec34615b-740a-48c1-8961-be5e59067605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1930102605124486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Security and Privacy in Data Engineering solution",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
